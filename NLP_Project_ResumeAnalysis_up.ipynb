{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read the skills column of the person and put it in a list\n",
    "import pandas as pd\n",
    "import re\n",
    "df = pd.read_csv(\"op1.csv\")\n",
    "s = df['Skills'].tolist()\n",
    "\n",
    "\n",
    "#m is the total list, of all the skills all appended together\n",
    "m = []\n",
    "for i in s:\n",
    "    str1 = i.split(',')\n",
    "    for j in str1:\n",
    "        m.append(j.lower())\n",
    "\n",
    "#m is the total list, of all the skills all appended together, one person at a time\n",
    "sentences = []\n",
    "for i in s:\n",
    "    temp = i.split(',')\n",
    "    l = []\n",
    "    for j in temp:\n",
    "          l.append(j.lower())\n",
    "    sentences.append(l)\n",
    "    \n",
    "\n",
    "#remove some crap entry\n",
    "m = filter(lambda a: a != '', m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Counter to see how many times a skill appears to give initial weightage to a skill\n",
    "from collections import Counter\n",
    "counts = Counter(m)\n",
    "countsfinal= []\n",
    "for i in counts.items():\n",
    "    l = []\n",
    "    l.append(i[0])\n",
    "    l.append(i[1])\n",
    "    countsfinal.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Normalize initial weights for each skill by using (1-val)/max and convert to dictionary format\n",
    "countsfinal.sort(key = lambda x: x[1])\n",
    "dict_weight = []\n",
    "for i in countsfinal:\n",
    "    init_weight = []\n",
    "    init_weight.append(i[0])\n",
    "    init_weight.append(1-(float(i[1])/countsfinal[-1][1]))\n",
    "    dict_weight.append(init_weight)\n",
    "d_init = dict(dict_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.423469387755\n"
     ]
    }
   ],
   "source": [
    "print(d_init[\"machine learning\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#INITIAL WEIGHTAGES DONE\n",
    "#WORD2VEC MODEL PART STARTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.22) or chardet (2.3.0) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "#To see how much to increase the weightage of the skill we use wordToVec embeddings for each word \n",
    "#by correlating the vals value with the index of the skill.\n",
    "import numpy\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences, min_count=10, iter = 20)\n",
    "\n",
    "skills = []\n",
    "for i in s:\n",
    "    str1 = i.split(',')\n",
    "    for j in str1:\n",
    "        skills.append(j.lower())\n",
    "\n",
    "vals = []\n",
    "for i in skills:\n",
    "    vals.append(numpy.linalg.norm(model[i])/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Normalize the change in weights so that it is more noticeable in the final result\n",
    "min1 = min(vals)\n",
    "max1 = max(vals)\n",
    "\n",
    "add_weight = []\n",
    "for i in skills:\n",
    "    a = []\n",
    "    a.append(i)\n",
    "    a.append((numpy.linalg.norm(model[i])/100 - min1)/(max1-min1) )\n",
    "    add_weight.append(a)\n",
    "dic_add = dict(add_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88528344515312918"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_add['data analytics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sk_set = list(set(skills))\n",
    "#sk_set\n",
    "len(sk_set)\n",
    "mat = []\n",
    "for i in sk_set:\n",
    "    mat.append(model[i])\n",
    "    \n",
    "#print(mat[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model['machine learning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#END OF WORD2VEC\n",
    "#PCA AND SIMILARITY MATCHING STARTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(194, 15)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "my_model = PCA(n_components=0.95, svd_solver='full') #this is as good as taking 95% variance\n",
    "m1 = my_model.fit_transform(mat)\n",
    "\n",
    "print(m1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "matrix = cosine_similarity(m1, m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "removed = []\n",
    "buckets = []\n",
    "for i in range (0,matrix.shape[0]):\n",
    "    l1 = []\n",
    "    for j in range (i,matrix.shape[0]):\n",
    "        if matrix[i][j] > 0.9 and j not in removed:\n",
    "            l1.append(sk_set[j])\n",
    "            removed.append(j)\n",
    "    buckets.append(l1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['nlp', 'keras', 'clustering', 'statistics'],\n",
       " ['network security',\n",
       "  'virtualisation',\n",
       "  'mips',\n",
       "  'ipv4',\n",
       "  'ipv6',\n",
       "  'assembly level language',\n",
       "  'http'],\n",
       " ['nosql'],\n",
       " ['kde', 'opensuse'],\n",
       " ['kafka', 'hbase', 'mapreduce', 'hive', 'ant'],\n",
       " ['kali-linux', 'emacs', 'make'],\n",
       " ['go', 'oop'],\n",
       " ['openstack', 'nova', 'dbaas'],\n",
       " [],\n",
       " ['xml', 'soa', 'eai', 'foaf'],\n",
       " ['makeitopen', 'fresco', 'react native'],\n",
       " ['arduino',\n",
       "  'microprocessor',\n",
       "  'pinguino',\n",
       "  'nanode',\n",
       "  'cypress',\n",
       "  'microcontroller'],\n",
       " ['flask', 'html', 'jquery', 'nodejs', 'interfacing with cloud apis'],\n",
       " ['database-basked web sites',\n",
       "  'angular2',\n",
       "  'meanstack',\n",
       "  'css',\n",
       "  'cloud computing'],\n",
       " ['socket programming', 'web2.0', 'iot', 'wireless'],\n",
       " ['flash', 'reactjs', 'javascript'],\n",
       " ['selenium', 'visualisation', 'neural networks', 'classification'],\n",
       " ['shimmer', 'pathpicker', 'stetho'],\n",
       " ['angularjs', 'web semantics'],\n",
       " ['agile methodologies', 'sdlc', 'product management', 'program management'],\n",
       " [],\n",
       " [],\n",
       " ['sap crm',\n",
       "  'integration',\n",
       "  'resource management',\n",
       "  'requirements analysis',\n",
       "  'business intelligence'],\n",
       " ['data mining',\n",
       "  'database management',\n",
       "  'data analytics',\n",
       "  'torch',\n",
       "  'chromium',\n",
       "  'artificial intelligence',\n",
       "  'machine learning'],\n",
       " [],\n",
       " [],\n",
       " ['python'],\n",
       " ['rebound', 'redex'],\n",
       " ['owl-s', 'mom', 'skos', 'rdf', 'rss', 'sparql', 'rdf+'],\n",
       " ['software forge', 'c++', 'automation'],\n",
       " ['risc'],\n",
       " ['knowledge management'],\n",
       " ['dart', 'uml', 'agile', 'fuchsia', 'perl'],\n",
       " ['ubuntu'],\n",
       " [],\n",
       " ['voip', 'cisc', 'internet of things'],\n",
       " ['similarity analysis'],\n",
       " ['.net'],\n",
       " ['zookeeper'],\n",
       " [],\n",
       " ['embedded operating systems'],\n",
       " ['digital ocean', 'bigdata', 'spark', 'tomcat', 'lens'],\n",
       " ['security', 'processor design'],\n",
       " ['transistors'],\n",
       " ['analysis of file system variations'],\n",
       " ['ajax', 'html5', 'django'],\n",
       " [],\n",
       " ['crm', 'leadership'],\n",
       " ['tensorflow'],\n",
       " [],\n",
       " [],\n",
       " ['enterprise software'],\n",
       " [],\n",
       " ['information design'],\n",
       " [],\n",
       " ['arm'],\n",
       " [],\n",
       " ['nano', 'gnu'],\n",
       " ['scala', 'java', 'scrum'],\n",
       " [],\n",
       " ['saas'],\n",
       " ['ambari', 'nutch', 'flume', 'aws'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['raspberry pi', 'x86'],\n",
       " ['3d modeling and animation'],\n",
       " [],\n",
       " ['product development'],\n",
       " ['business analysis'],\n",
       " [],\n",
       " [],\n",
       " ['eii', 'etl'],\n",
       " [],\n",
       " ['git'],\n",
       " ['procedural generation of scenes'],\n",
       " [],\n",
       " ['debian'],\n",
       " [],\n",
       " ['swift', 'iaas'],\n",
       " ['android studio', 'app inventor'],\n",
       " ['buck', 'conceal'],\n",
       " ['c'],\n",
       " [],\n",
       " ['gnome', 'windows'],\n",
       " [],\n",
       " [],\n",
       " ['mobility', 'lan'],\n",
       " [],\n",
       " ['cinder', 'cryptocurrency'],\n",
       " ['virtual reality'],\n",
       " [],\n",
       " ['bolts-android'],\n",
       " ['rdfs'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['software configuration management'],\n",
       " ['system modelling'],\n",
       " ['vim', 'linux', 'fuse'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['3d computer graphics'],\n",
       " [],\n",
       " [],\n",
       " ['adaptation of existing file systems'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['vr', 'ar', 'nvidia'],\n",
       " [],\n",
       " ['labview', 'r', 'algorithms'],\n",
       " [],\n",
       " ['graphql', 'sql'],\n",
       " ['operating systems'],\n",
       " [],\n",
       " ['3d data visualization'],\n",
       " ['impala'],\n",
       " [],\n",
       " [],\n",
       " ['file system utilities'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['cassandra'],\n",
       " ['software engineering'],\n",
       " [],\n",
       " [],\n",
       " ['hadoop'],\n",
       " [],\n",
       " ['natural language processing', 'nltk'],\n",
       " ['bibtex'],\n",
       " ['android'],\n",
       " [],\n",
       " [],\n",
       " ['infer'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['mongodb'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['dropbox'],\n",
       " [],\n",
       " ['paas'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#In Case we take data scraped from linkedin or enter skills and Description separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_skills = ['machine learning','data analytics','hadoop','spark']\n",
    "test_desc = 'Our project involves Machine Learning for Fast Optimal Search . It is implemented using Flask IT Policy Procedures Foundation'\n",
    "accepted_buckets = []\n",
    "\n",
    "#Finding those buckets that have the skills from test_skills\n",
    "for i in test_skills:\n",
    "    for j in range(0,len(buckets)):\n",
    "        if i in buckets[j]:\n",
    "            accepted_buckets.append(j)\n",
    "            \n",
    "#Appending the skills of the person with the related skills\n",
    "#As a list of list so that final values of only appropriate set of skills are updated\n",
    "final_pskills = []\n",
    "for i in accepted_buckets:\n",
    "    final_pskills.append(buckets[i])\n",
    "    \n",
    "#Check for 2 skills in same category, list gets added twice\n",
    "final_pskills = [list(x) for x in set(tuple(x) for x in final_pskills)]\n",
    "\n",
    "#Appending the skills of the person with the related skills as a list\n",
    "test_skills = []\n",
    "for i in accepted_buckets:\n",
    "    test_skills.extend(buckets[i])\n",
    "    \n",
    "test_skills = list(set(test_skills))\n",
    "\n",
    "#Converting provided description of project to lower case\n",
    "test_desc = test_desc.lower()\n",
    "\n",
    "#Append to a dictionay as key value pairs where key = skillname, value = its initial weightage\n",
    "person_skill = []\n",
    "for j in test_skills: \n",
    "    if j != '':\n",
    "        p = []\n",
    "        p.append(j)\n",
    "        p.append(d_init[j])\n",
    "        person_skill.append(p)\n",
    "person_skill = dict(person_skill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'artificial intelligence': 0.41326530612244894,\n",
       " 'bigdata': 0.44387755102040816,\n",
       " 'chromium': 0.44047619047619047,\n",
       " 'data analytics': 0.44557823129251706,\n",
       " 'data mining': 0.44557823129251706,\n",
       " 'database management': 0.45408163265306123,\n",
       " 'digital ocean': 0.3928571428571429,\n",
       " 'hadoop': 0.40476190476190477,\n",
       " 'lens': 0.41836734693877553,\n",
       " 'machine learning': 0.423469387755102,\n",
       " 'spark': 0.4387755102040817,\n",
       " 'tomcat': 0.435374149659864,\n",
       " 'torch': 0.4302721088435374}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#These are the initial weights for all skills related to those given in test_skills field\n",
    "person_skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Display final results using string matching and appropriate initial value and word2vec weights\n",
    "#Append weights for all related skills if the skill is found in the description\n",
    "for i in range (0,len(test_skills)):\n",
    "    if re.search(test_skills[i],test_desc):\n",
    "        for j in final_pskills:\n",
    "            if test_skills[i] in j:\n",
    "                for kk in j:\n",
    "                    person_skill[kk] = person_skill[kk] + dic_add[kk]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('digital ocean', 0.3928571428571429)\n",
      "('hadoop', 0.40476190476190477)\n",
      "('lens', 0.41836734693877553)\n",
      "('tomcat', 0.435374149659864)\n",
      "('spark', 0.4387755102040817)\n",
      "('bigdata', 0.44387755102040816)\n",
      "('machine learning', 1.1191666502076822)\n",
      "('torch', 1.1905071771114941)\n",
      "('data mining', 1.2009438799067866)\n",
      "('artificial intelligence', 1.2037065028596408)\n",
      "('database management', 1.2092227715179447)\n",
      "('chromium', 1.287414026469176)\n",
      "('data analytics', 1.3308616764456462)\n"
     ]
    }
   ],
   "source": [
    "#Displaying Updated skills beautifully\n",
    "person_skill = sorted(person_skill.iteritems(), key=lambda i: i[1])\n",
    "for i in person_skill:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#If we want to read from PDF and do the same\n",
    "#A person can give the skills he is looking for in a particular field in the test_skills field\n",
    "#We parse the PDF of the person's resume as a text and look for the appropriate skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cStringIO import StringIO\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import os\n",
    "import sys, getopt\n",
    "\n",
    "#converts pdf, returns its text content as a string\n",
    "def convert(fname, pages=None):\n",
    "    if not pages:\n",
    "        pagenums = set()\n",
    "    else:\n",
    "        pagenums = set(pages)\n",
    "\n",
    "    output = StringIO()\n",
    "    manager = PDFResourceManager()\n",
    "    converter = TextConverter(manager, output, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(manager, converter)\n",
    "\n",
    "    infile = file(fname, 'rb')\n",
    "    for page in PDFPage.get_pages(infile, pagenums):\n",
    "        interpreter.process_page(page)\n",
    "    infile.close()\n",
    "    converter.close()\n",
    "    text = output.getvalue()\n",
    "    output.close\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used my resume for testing, Has nothing related to bigdata\n",
    "#SO all the skills associated with spark will not have an increase in weightages\n",
    "test_skills = ['labview','spark']\n",
    "test_desc = convert (\"Resume/01FB15ECS247.pdf\")\n",
    "accepted_buckets = []\n",
    "\n",
    "#Finding those buckets that have the skills from test_skills\n",
    "for i in test_skills:\n",
    "    for j in range(0,len(buckets)):\n",
    "        if i in buckets[j]:\n",
    "            accepted_buckets.append(j)\n",
    "            \n",
    "#Appending the skills of the person with the related skills\n",
    "#As a list of list so that final values of only appropriate set of skills are updated\n",
    "final_pskills = []\n",
    "for i in accepted_buckets:\n",
    "    final_pskills.append(buckets[i])\n",
    "    \n",
    "#Check for 2 skills in same category, list gets added twice\n",
    "final_pskills = [list(x) for x in set(tuple(x) for x in final_pskills)]\n",
    "\n",
    "#Appending the skills of the person with the related skills as a list\n",
    "test_skills = []\n",
    "for i in accepted_buckets:\n",
    "    test_skills.extend(buckets[i])\n",
    "\n",
    "test_skills = list(set(test_skills))\n",
    "\n",
    "#Converting provided description of project to lower case\n",
    "test_desc = test_desc.lower()\n",
    "\n",
    "#Append to a dictionay as key value pairs where key = skillname, value = its initial weightage\n",
    "person_skill = []\n",
    "for j in test_skills: \n",
    "    if j != '':\n",
    "        p = []\n",
    "        p.append(j)\n",
    "        p.append(d_init[j])\n",
    "        person_skill.append(p)\n",
    "person_skill = dict(person_skill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['digital ocean', 'bigdata', 'spark', 'tomcat', 'lens'],\n",
       " ['labview', 'r', 'algorithms']]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pskills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithms': 0.44387755102040816,\n",
       " 'bigdata': 0.44387755102040816,\n",
       " 'digital ocean': 0.3928571428571429,\n",
       " 'labview': 0.4302721088435374,\n",
       " 'lens': 0.41836734693877553,\n",
       " 'r': 0.4319727891156463,\n",
       " 'spark': 0.4387755102040817,\n",
       " 'tomcat': 0.435374149659864}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display final results using string matching and appropriate initial value and word2vec weights\n",
    "#Append weights for all related skills if the skill is found in the description\n",
    "for i in range (0,len(test_skills)):\n",
    "    if re.search(' '+test_skills[i]+ ' ',test_desc):\n",
    "        for j in final_pskills:\n",
    "            if test_skills[i] in j:\n",
    "                for kk in j:\n",
    "                    person_skill[kk] = person_skill[kk] + dic_add[kk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('digital ocean', 0.3928571428571429)\n",
      "('lens', 0.41836734693877553)\n",
      "('tomcat', 0.435374149659864)\n",
      "('spark', 0.4387755102040817)\n",
      "('bigdata', 0.44387755102040816)\n",
      "('r', 2.6645753222580546)\n",
      "('labview', 2.6855884864971817)\n",
      "('algorithms', 2.8468870841654139)\n"
     ]
    }
   ],
   "source": [
    "#Displaying Updated skills beautifully\n",
    "person_skill = sorted(person_skill.iteritems(), key=lambda i: i[1])\n",
    "for i in person_skill:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
